from dotenv import (
    find_dotenv,
    load_dotenv,
)


load_dotenv(find_dotenv(), override=True)

import tempfile
from typing import (
    AsyncGenerator,
    Dict,
    Generator,
    List,
)

import pytest
import pytest_asyncio
from scipy.spatial import distance

from langchain_community.vectorstores import Chroma
from langchain_core.documents import Document
from langchain_openai import OpenAIEmbeddings

from load_document import load_document
from multi_vector_document_db import MultiVectorDocumentDB
from multi_vectorstore import _chunk
from test_document_db import InMemoryVectorStore


DOCS = [
    "../examples/files/state_of_the_union.txt",
    "../examples/files/us_constitution.pdf",
]
LARGE_CHUNK_SIZE = 4000
SMALL_CHUNK_SIZE = 1000
EMBEDDINGS = OpenAIEmbeddings()
QUERY = "what is the state of the union?"
EMBEDDED_QUERY = EMBEDDINGS.embed_query(QUERY)


# Functions to replicate the transformation of parent documets by the MultiVectorStore
def chunk(doc: Document, **kwargs) -> List[Document]:
    return _chunk(doc, metadata={}, **kwargs)


def total_len(docs: List[Document]) -> int:
    return sum([len(doc.page_content) for doc in docs])


# This chunked content will be our parent documents
@pytest.fixture(scope="function")
def docs() -> List[Document]:
    chunks = []
    for doc_path in DOCS:
        chunks.extend(
            load_document(
                doc_path,
                chunk_it=True,
                chunk_size=LARGE_CHUNK_SIZE,
                chunk_overlap=LARGE_CHUNK_SIZE * 0.1,
            )
        )
    return chunks


@pytest.fixture(scope="function")
def docs_per_source(docs: List[Document]) -> Dict:
    docs_per_source = {}
    for doc in docs:
        if doc.metadata["source"] not in docs_per_source:
            docs_per_source[doc.metadata["source"]] = []
        docs_per_source[doc.metadata["source"]].append(doc)
    return docs_per_source


# Child documents equivalent to the ones generated by the MultiVectorStore
@pytest.fixture(scope="function")
def sub_docs_chunk(docs: List[Document]) -> List[Document]:
    sub_docs = []
    for doc in docs:
        sub_docs.extend(
            chunk(
                doc, chunk_size=SMALL_CHUNK_SIZE, chunk_overlap=SMALL_CHUNK_SIZE * 0.1
            )
        )
    return sub_docs


# Euclidean distances of the child documents to the query to test the retriever for the MultiVectorStore
@pytest.fixture(scope="function")
def sub_docs_distances(sub_docs_chunk: List[Document]) -> List[float]:
    embedded_sub_docs = [
        EMBEDDINGS.embed_query(sub_doc.page_content) for sub_doc in sub_docs_chunk
    ]
    distances = [
        distance.euclidean(EMBEDDED_QUERY, embedded_sub_doc)
        for embedded_sub_doc in embedded_sub_docs
    ]
    distances.sort()
    return distances


@pytest.fixture(scope="function")
def document_db() -> Generator[MultiVectorDocumentDB, None, None]:
    """Document DB fixture."""
    # Create a temporary directory for the test database
    temp_dir = tempfile.mkdtemp()
    document_db = MultiVectorDocumentDB.create(
        location=temp_dir,
        vectorstore=InMemoryVectorStore(),
        db_url="sqlite:///:memory:",
        functor="chunk",
        func_kwargs={
            "chunk_size": SMALL_CHUNK_SIZE,
            "chunk_overlap": SMALL_CHUNK_SIZE * 0.1,
        },
    )
    yield document_db
    # Cleanup after tests
    document_db.delete_index()


@pytest.fixture(scope="function")
def db_with_search() -> Generator[MultiVectorDocumentDB, None, None]:
    """Document DB fixture."""
    # Create a temporary directory for the test database
    temp_dir = tempfile.mkdtemp()
    document_db = MultiVectorDocumentDB.create(
        location=temp_dir,
        vectorstore=Chroma(embedding_function=EMBEDDINGS),
        db_url="sqlite:///:memory:",
        functor="chunk",
        func_kwargs={
            "chunk_size": SMALL_CHUNK_SIZE,
            "chunk_overlap": SMALL_CHUNK_SIZE * 0.1,
        },
    )
    yield document_db
    # Cleanup after tests
    document_db.delete_index()


@pytest_asyncio.fixture
async def adocument_db() -> AsyncGenerator[MultiVectorDocumentDB, None]:
    """Async Document DB fixture."""
    # Create a temporary directory for the test database
    temp_dir = tempfile.mkdtemp()
    adocument_db = await MultiVectorDocumentDB.acreate(
        location=temp_dir,
        vectorstore=InMemoryVectorStore(),
        db_url="sqlite+aiosqlite:///:memory:",
        functor="chunk",
        func_kwargs={
            "chunk_size": SMALL_CHUNK_SIZE,
            "chunk_overlap": SMALL_CHUNK_SIZE * 0.1,
        },
    )
    yield adocument_db
    # Cleanup after tests
    adocument_db.delete_index()


@pytest_asyncio.fixture
async def adb_with_search() -> AsyncGenerator[MultiVectorDocumentDB, None]:
    """Async Document DB fixture."""
    # Create a temporary directory for the test database
    temp_dir = tempfile.mkdtemp()
    adocument_db = await MultiVectorDocumentDB.acreate(
        location=temp_dir,
        vectorstore=Chroma(embedding_function=EMBEDDINGS),
        db_url="sqlite+aiosqlite:///:memory:",
        functor="chunk",
        func_kwargs={
            "chunk_size": SMALL_CHUNK_SIZE,
            "chunk_overlap": SMALL_CHUNK_SIZE * 0.1,
        },
    )
    yield adocument_db
    # Cleanup after tests
    adocument_db.delete_index()


def test_upserting(
    document_db: MultiVectorDocumentDB,
    docs: List[Document],
    sub_docs_chunk: List[Document],
) -> None:
    """Upserting some content runs the transformation functor and inserts parent and child docs."""
    assert document_db.upsert_documents(docs) == {
        "num_added": len(docs),
        "num_deleted": 0,
        "num_skipped": 0,
        "num_updated": 0,
    }

    # Verify thet the parent docs are in the DB
    assert len(
        document_db.vectorstore.docstore.mget(
            document_db.vectorstore.docstore.yield_keys()
        )
    ) == len(docs)
    assert total_len(
        document_db.vectorstore.docstore.mget(
            document_db.vectorstore.docstore.yield_keys()
        )
    ) == total_len(docs)

    # using in memory implementation here
    assert isinstance(document_db.vectorstore.vectorstore, InMemoryVectorStore)

    # Verify thet the child docs are in the DB
    assert len(document_db.vectorstore.vectorstore.store) == len(sub_docs_chunk)
    assert total_len(document_db.vectorstore.vectorstore.store.values()) == total_len(
        sub_docs_chunk
    )


@pytest.mark.requires("aiosqlite")
@pytest.mark.asyncio
async def test_aupserting(
    adocument_db: MultiVectorDocumentDB,
    docs: List[Document],
    sub_docs_chunk: List[Document],
) -> None:
    """Upserting some content runs the transformation functor and inserts parent and child docs."""
    assert await adocument_db.aupsert_documents(docs) == {
        "num_added": len(docs),
        "num_deleted": 0,
        "num_skipped": 0,
        "num_updated": 0,
    }

    # Verify thet the parent docs are in the DB
    assert len(
        adocument_db.vectorstore.docstore.mget(
            adocument_db.vectorstore.docstore.yield_keys()
        )
    ) == len(docs)
    assert total_len(
        adocument_db.vectorstore.docstore.mget(
            adocument_db.vectorstore.docstore.yield_keys()
        )
    ) == total_len(docs)

    # using in memory implementation here
    assert isinstance(adocument_db.vectorstore.vectorstore, InMemoryVectorStore)

    # Verify thet the child docs are in the DB
    assert len(adocument_db.vectorstore.vectorstore.store) == len(sub_docs_chunk)
    assert total_len(
        list(adocument_db.vectorstore.vectorstore.store.values())
    ) == total_len(sub_docs_chunk)


def test_upserting_same_content(
    document_db: MultiVectorDocumentDB,
    docs: List[Document],
    sub_docs_chunk: List[Document],
) -> None:
    """Upserting some content twice to confirm it gets added only once."""
    assert document_db.upsert_documents(docs) == {
        "num_added": len(docs),
        "num_deleted": 0,
        "num_skipped": 0,
        "num_updated": 0,
    }

    # Restore the original content
    for doc in docs:
        doc.metadata.pop("id")

    # Insert the same content again, verify it doesn't get added again
    assert document_db.upsert_documents(docs) == {
        "num_added": 0,
        "num_deleted": 0,
        "num_skipped": len(docs),
        "num_updated": 0,
    }


@pytest.mark.requires("aiosqlite")
@pytest.mark.asyncio
async def test_aupserting_same_content(
    adocument_db: MultiVectorDocumentDB,
    docs: List[Document],
    sub_docs_chunk: List[Document],
) -> None:
    """Upserting some content twice to confirm it gets added only once."""
    assert await adocument_db.aupsert_documents(docs) == {
        "num_added": len(docs),
        "num_deleted": 0,
        "num_skipped": 0,
        "num_updated": 0,
    }

    # Restore the original content
    for doc in docs:
        doc.metadata.pop("id")

    # Insert the same content again, verify it doesn't get added again
    assert await adocument_db.aupsert_documents(docs) == {
        "num_added": 0,
        "num_deleted": 0,
        "num_skipped": len(docs),
        "num_updated": 0,
    }


def test_upserting_deletes(
    document_db: MultiVectorDocumentDB, docs: List[Document], docs_per_source: Dict
) -> None:
    """Test upserting updated documents results in deletion."""
    assert document_db.upsert_documents(docs) == {
        "num_added": len(docs),
        "num_deleted": 0,
        "num_skipped": 0,
        "num_updated": 0,
    }

    # Restore the original content
    for doc in docs:
        print(doc.metadata)
        doc.metadata.pop("id")

    # Keep 1 document from a source and create 2 documents from the same source all with mutated content
    docs2 = [
        docs[0],
        Document(
            page_content="mutated document 1",
            metadata={"source": docs[0].metadata["source"]},
        ),
        Document(
            page_content="mutated document 2",
            metadata={"source": docs[0].metadata["source"]},
        ),
    ]

    # Upsert the new documents and verify that the outdated documents are deleted
    assert document_db.upsert_documents(docs2) == {
        "num_added": 2,
        "num_deleted": len(docs_per_source[docs[0].metadata["source"]]) - 1,
        "num_skipped": 1,
        "num_updated": 0,
    }


@pytest.mark.requires("aiosqlite")
@pytest.mark.asyncio
async def test_aupserting_deletes(
    adocument_db: MultiVectorDocumentDB, docs: List[Document], docs_per_source: Dict
) -> None:
    """Test upserting updated documents results in deletion."""
    assert await adocument_db.aupsert_documents(docs) == {
        "num_added": len(docs),
        "num_deleted": 0,
        "num_skipped": 0,
        "num_updated": 0,
    }

    # Restore the original content
    for doc in docs:
        doc.metadata.pop("id")

    # Keep 1 document from a source and create 2 documents from the same source all with mutated content
    docs2 = [
        docs[0],
        Document(
            page_content="mutated document 2",
            metadata={"source": docs[0].metadata["source"]},
        ),
        Document(
            page_content="This is another document.",  # <-- Same as original
            metadata={"source": docs[0].metadata["source"]},
        ),
    ]

    # Upsert the new documents and verify that the outdated documents are deleted
    assert await adocument_db.aupsert_documents(docs2) == {
        "num_added": 2,
        "num_deleted": len(docs_per_source[docs[0].metadata["source"]]) - 1,
        "num_skipped": 1,
        "num_updated": 0,
    }


def test_deduplication(
    document_db: MultiVectorDocumentDB, docs: List[Document]
) -> None:
    """Check that duplicates are not added."""
    more_docs = docs + [docs[0]]

    # Should result in only a single copy of each document being added
    assert document_db.upsert_documents(more_docs) == {
        "num_added": len(docs),
        "num_deleted": 0,
        "num_skipped": 0,
        "num_updated": 0,
    }


@pytest.mark.requires("aiosqlite")
@pytest.mark.asyncio
async def test_adeduplication(
    adocument_db: MultiVectorDocumentDB, docs: List[Document]
) -> None:
    """Check that duplicates are not added."""
    more_docs = docs + [docs[0]]

    # Should result in only a single copy of each document being added
    assert await adocument_db.aupsert_documents(more_docs) == {
        "num_added": len(docs),
        "num_deleted": 0,
        "num_skipped": 0,
        "num_updated": 0,
    }


def test_delete(
    document_db: MultiVectorDocumentDB, docs: List[Document], docs_per_source: Dict
) -> None:
    """Test that the delete method functions as expected."""
    assert document_db.upsert_documents(docs) == {
        "num_added": len(docs),
        "num_deleted": 0,
        "num_skipped": 0,
        "num_updated": 0,
    }

    to_delete = docs[0].metadata["source"]

    # Very that all the documents from the source are deleted
    assert document_db.delete_documents([to_delete]) == {
        "num_added": 1,
        "num_deleted": len(docs_per_source[to_delete]),
        "num_skipped": 0,
        "num_updated": 0,
    }

    # using in memory implementation here
    assert isinstance(document_db.vectorstore.vectorstore, InMemoryVectorStore)

    # Very that the documents are actually deleted
    contents = sorted(
        [
            document.page_content
            for document in document_db.vectorstore.vectorstore.store.values()
            if document.metadata["source"] == to_delete
        ]
    )
    assert contents == ["Deleted DO NOT USE"]


@pytest.mark.requires("aiosqlite")
@pytest.mark.asyncio
async def test_adelete(
    adocument_db: MultiVectorDocumentDB, docs: List[Document], docs_per_source: Dict
) -> None:
    """Test that the delete method functions as expected."""
    assert await adocument_db.aupsert_documents(docs) == {
        "num_added": len(docs),
        "num_deleted": 0,
        "num_skipped": 0,
        "num_updated": 0,
    }

    to_delete = docs[0].metadata["source"]

    # Very that all the documents from the source are deleted
    assert await adocument_db.adelete_documents([to_delete]) == {
        "num_added": 1,
        "num_deleted": len(docs_per_source[to_delete]),
        "num_skipped": 0,
        "num_updated": 0,
    }

    # using in memory implementation here
    assert isinstance(adocument_db.vectorstore.vectorstore, InMemoryVectorStore)

    # Very that the documents are actually deleted
    contents = sorted(
        [
            document.page_content
            for document in adocument_db.vectorstore.vectorstore.store.values()
            if document.metadata["source"] == to_delete
        ]
    )
    assert contents == ["Deleted DO NOT USE"]


def test_clean(document_db: MultiVectorDocumentDB, docs: List[Document]) -> None:
    """Test that the clean method functions as expected."""
    document_db.upsert_documents(docs)

    # Clean should not delete anything
    assert document_db.clean() == {
        "num_added": 0,
        "num_deleted": len(docs),
        "num_skipped": 0,
        "num_updated": 0,
    }

    # Very that the parent documents are actually deleted
    assert (
        document_db.vectorstore.docstore.mget(
            document_db.vectorstore.docstore.yield_keys()
        )
        == []
    )

    # using in memory implementation here
    assert isinstance(document_db.vectorstore.vectorstore, InMemoryVectorStore)

    # Very that the child documents are actually deleted
    assert list(document_db.vectorstore.vectorstore.store.values()) == []


@pytest.mark.requires("aiosqlite")
@pytest.mark.asyncio
async def test_aclean(
    adocument_db: MultiVectorDocumentDB, docs: List[Document]
) -> None:
    """Test that the clean method functions as expected."""
    await adocument_db.aupsert_documents(docs)

    # Clean should not delete anything
    assert await adocument_db.aclean() == {
        "num_added": 0,
        "num_deleted": len(docs),
        "num_skipped": 0,
        "num_updated": 0,
    }

    # Very that the parent documents are actually deleted
    assert (
        adocument_db.vectorstore.docstore.mget(
            adocument_db.vectorstore.docstore.yield_keys()
        )
        == []
    )

    # using in memory implementation here
    assert isinstance(adocument_db.vectorstore.vectorstore, InMemoryVectorStore)

    # Very that the child documents are actually deleted
    assert list(adocument_db.vectorstore.vectorstore.store.values()) == []


def test_retriever(
    db_with_search: MultiVectorDocumentDB,
    docs: List[Document],
    sub_docs_distances: List[float],
) -> None:
    """Test the retriever for the MultiVectorDocumentDB."""
    # Insert the parente documents and generate the child documents
    db_with_search.upsert_documents(docs)

    # Retrive the parent document of the most similar child document in comparison with the query
    retriever = db_with_search.as_retriever(search_kwargs={"k": 1})
    results = retriever.invoke(QUERY)

    # Since we only searched for one child, the retriver should return only one parent document
    assert len(results) == 1

    # Generate child focuments for the parent document retrieved
    sub_docs = chunk(
        results[0], chunk_size=SMALL_CHUNK_SIZE, chunk_overlap=SMALL_CHUNK_SIZE * 0.1
    )

    # Make sure that the most similar child document is one of the child documents generated
    for sub_doc in sub_docs:
        dist = distance.euclidean(
            EMBEDDINGS.embed_query(sub_doc.page_content), EMBEDDED_QUERY
        )
        if dist == pytest.approx(sub_docs_distances[0], abs=1e-3):
            return
    assert False


@pytest.mark.requires("aiosqlite")
@pytest.mark.asyncio
async def test_aretriever(
    adb_with_search: MultiVectorDocumentDB,
    docs: List[Document],
    sub_docs_distances: List[float],
) -> None:
    """Test the retriever for the MultiVectorDocumentDB."""
    # Insert the parente documents and generate the child documents
    await adb_with_search.aupsert_documents(docs)

    # Retrive the parent document of the most similar child document in comparison with the query
    retriever = adb_with_search.as_retriever(search_kwargs={"k": 1})
    results = await retriever.ainvoke(QUERY)

    # Since we only searched for one child, the retriver should return only one parent document
    assert len(results) == 1

    # Generate child focuments for the parent document retrieved
    sub_docs = chunk(
        results[0], chunk_size=SMALL_CHUNK_SIZE, chunk_overlap=SMALL_CHUNK_SIZE * 0.1
    )

    # Make sure that the most similar child document is one of the child documents generated
    for sub_doc in sub_docs:
        dist = distance.euclidean(
            EMBEDDINGS.embed_query(sub_doc.page_content), EMBEDDED_QUERY
        )
        if dist == pytest.approx(sub_docs_distances[0], abs=1e-3):
            return
    assert False
